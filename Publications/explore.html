#!/usr/bin/env python3
from pathlib import Path
import re
import json

BIB_FILE = Path("Publications/references.bib")     # <-- your single source of truth
OUT_MD   = Path("Publications/publications.md")
OUT_JSON = Path("assets/data/pubs.json")           # optional; handy later
OUT_GRAPH = Path("assets/data/topic_graph.json")

def tags_from_keywords(e: dict) -> list[str]:
    kw = (e.get("keywords") or e.get("keyword") or "")
    parts = re.split(r"[;,]", kw)
    return [p.strip() for p in parts if p.strip()]

def build_topic_graph(pubs: list[dict]) -> dict:
    # Build a co-occurrence graph from pubs[*].tags (lowercased)
    tag_set = set()
    edge_w  = {}
    for p in pubs:
        t = [str(s).strip().lower() for s in (p.get("tags") or []) if str(s).strip()]
        t = sorted(set(t))
        for a in t:
            tag_set.add(a)
        for i in range(len(t)):
            for j in range(i+1, len(t)):
                key = (t[i], t[j])
                edge_w[key] = edge_w.get(key, 0) + 1

    nodes = [{"id": t} for t in sorted(tag_set)]
    links = [{"source": a, "target": b, "weight": w} for (a,b), w in edge_w.items()]
    return {"nodes": nodes, "links": links}

# -------- NEW: enrich graph with counts, weighted degree, and communities --------
def enrich_topic_graph(graph: dict, pubs: list[dict]) -> dict:
    """
    Returns a NEW graph dict with added fields on nodes:
      - count:     number of papers that contain this tag
      - degree_w:  sum of incident link weights
      - community: Louvain community id (fallback to 0 if unavailable)
    """
    # --- counts from pubs.json ---
    tag_count: dict[str, int] = {}
    for p in pubs:
        for t in (p.get("tags") or []):
            k = str(t).strip().lower()
            if not k: 
                continue
            tag_count[k] = tag_count.get(k, 0) + 1

    # --- weighted degree from links ---
    deg_w: dict[str, int] = {}
    for l in (graph.get("links") or []):
        s = str(l.get("source"))
        t = str(l.get("target"))
        w = int(l.get("weight") or 1)
        deg_w[s] = deg_w.get(s, 0) + w
        deg_w[t] = deg_w.get(t, 0) + w

    # --- community detection (optional) ---
    partition: dict[str, int] = {}
    try:
        import networkx as nx  # type: ignore
        try:
            import community as community_louvain  # python-louvain
        except Exception:
            community_louvain = None

        if community_louvain is not None:
            G = nx.Graph()
            # ensure all nodes exist (even isolated)
            for n in (graph.get("nodes") or []):
                G.add_node(str(n["id"]))
            for l in (graph.get("links") or []):
                G.add_edge(str(l["source"]), str(l["target"]), weight=int(l.get("weight") or 1))
            # stable result via random_state
            partition = community_louvain.best_partition(G, weight="weight", random_state=42)
        else:
            # lib not available
            raise ImportError("python-louvain not installed")
    except Exception:
        # fallback: single community
        for n in (graph.get("nodes") or []):
            partition[str(n["id"])] = 0

    # --- stable renumbering of communities (0..K-1) by size (count sum desc) ---
    comm_bins: dict[int, list[str]] = {}
    for nid, cid in partition.items():
        comm_bins.setdefault(int(cid), []).append(nid)

    # score each community by (sum counts, then sum deg_w) for stable ordering
    def comm_score(cid: int):
        members = comm_bins[cid]
        s_count = sum(tag_count.get(m, 0) for m in members)
        s_deg   = sum(deg_w.get(m, 0) for m in members)
        return (-s_count, -s_deg, cid)

    order = sorted(comm_bins.keys(), key=comm_score)
    remap = {old: i for i, old in enumerate(order)}

    # --- build new node list with added fields ---
    new_nodes = []
    for n in (graph.get("nodes") or []):
        nid = str(n["id"]).strip().lower()
        new_nodes.append({
            "id": n["id"],
            "count": int(tag_count.get(nid, 0)),
            "degree_w": int(deg_w.get(nid, 0)),
            "community": int(remap.get(int(partition.get(nid, 0)), 0)),
        })

    # keep links as-is
    return {"nodes": new_nodes, "links": graph.get("links", [])}

# -------------------------------- existing bib parsing below --------------------------------
def read_bib() -> list[dict]:
    """
    Load Publications/references.bib.
    - Parse with bibtexparser (good unicode handling) but it may drop @preprint.
    - Also parse with a regex fallback that keeps ALL entry types.
    - Merge the two (dedupe on type+ID).
    """
    bib_path = Path("Publications/references.bib")
    txt = bib_path.read_text(encoding="utf-8")

    # --- regex fallback that keeps unknown types (e.g., @preprint) ---
    entries_rx: list[dict] = []
    entry_re = re.compile(r'@(\w+)\s*{\s*([^,]+)\s*,(.*?)\n}\s*', re.S | re.I)
    field_re = re.compile(r'([A-Za-z][A-Za-z0-9_-]*)\s*=\s*(\{(?:[^{}]|\{[^{}]*\})*\}|"[^"]*")\s*,?', re.S)

    for m in entry_re.finditer(txt):
        etype = m.group(1).strip().lower()
        key   = m.group(2).strip()
        body  = m.group(3)
        d = {"ENTRYTYPE": etype, "ID": key}
        for fm in field_re.finditer(body):
            k = fm.group(1).lower()
            v = fm.group(2).strip()
            if (v.startswith("{") and v.endswith("}")) or (v.startswith('"') and v.endswith('"')):
                v = v[1:-1]
            d[k] = v.strip()
        entries_rx.append(d)

    # --- bibtexparser parse (may print "preprint not standard" and drop it) ---
    entries_bp: list[dict] = []
    try:
        import bibtexparser
        from bibtexparser.bparser import BibTexParser
        parser = BibTexParser(common_strings=True)
        db = bibtexparser.loads(txt, parser=parser)
        entries_bp = db.entries or []
    except Exception:
        entries_bp = []

    # --- merge results (prefer bibtexparser entry when duplicate) ---
    def keyfn(e: dict):
        return ((e.get("ENTRYTYPE") or e.get("entrytype") or "").lower(),
                (e.get("ID") or e.get("id") or "").lower())

    out: list[dict] = []
    seen = set()

    for e in entries_bp + entries_rx:
        k = keyfn(e)
        if k in seen:
            continue
        seen.add(k)
        out.append(e)

    print(f"[read_bib] merged entries: total {len(out)} "
          f"(bibtexparser {len(entries_bp)}, regex {len(entries_rx)})")
    return out

def etype(e): return (e.get("ENTRYTYPE") or e.get("entrytype") or "").lower()
def year(e):
    y = str(e.get("year") or "")
    m = re.search(r"\d{4}", y)
    return int(m.group(0)) if m else 0
def is_journal(e: dict) -> bool:
    return (not is_preprint(e)) and (etype(e) == "article" or bool(e.get("journal") or e.get("journaltitle")))
def is_preprint(e: dict) -> bool:
    t = (e.get("ENTRYTYPE") or e.get("entrytype") or "").lower()
    if t in ("preprint", "unpublished"):
        return True
    note = ((e.get("note") or "") + " " + (e.get("howpublished") or "")).lower()
    if "preprint" in note:
        return True
    url = (e.get("url") or "").lower()
    if any(h in url for h in ("arxiv.org","ssrn.com","biorxiv.org","medrxiv.org","chemrxiv.org","osf.io")):
        return True
    ap = (e.get("archiveprefix") or e.get("archivePrefix") or "").lower()
    return ap == "arxiv"
def is_chapter(e):
    return etype(e) == "incollection"
def is_conf(e):
    return etype(e) == "inproceedings"
def authors_str(e):
    a = str(e.get("author") or "").replace("\n"," ")
    parts = [p.strip() for p in a.split(" and ") if p.strip()]
    return ", ".join(parts)
def title_str(e):
    t = str(e.get("title") or "").strip().strip("{}")
    t = t.replace("``", '"').replace("''", '"')
    return t
def venue_str(e: dict, kind: str) -> str:
    if kind == "journal":
        return (e.get("journal") or e.get("journaltitle") or "").strip()
    if kind == "preprint":
        u   = (e.get("url") or "").lower()
        ap  = (e.get("archiveprefix") or e.get("archivePrefix") or "").lower()
        eid = (e.get("eprint") or "").strip()
        if ap == "arxiv" or "arxiv.org" in u:
            return "arXiv" + (f":{eid}" if eid else "")
        if "ssrn.com" in u:     return "SSRN"
        if "biorxiv.org" in u:  return "bioRxiv"
        if "medrxiv.org" in u:  return "medRxiv"
        if "chemrxiv.org" in u: return "ChemRxiv"
        return ((e.get("note") or e.get("howpublished") or "Preprint")).strip()
    if kind in ("chapter", "conf"):
        return (e.get("booktitle") or "").strip()
    return ""
def link_str(e):
    url = str(e.get("url") or "").strip()
    if url: return f"[Link]({url})"
    doi = str(e.get("doi") or "").strip()
    if doi:
        if not doi.lower().startswith("http"):
            doi = "https://doi.org/" + doi
        return f"[Link]({doi})"
    return ""
def render_list(title, items, kind):
    lines = []
    lines.append(f"## {title}\n")
    if not items:
        lines.append("_No entries._\n")
        return lines
    items.sort(key=year, reverse=True)
    for i, e in enumerate(items, 1):
        au = authors_str(e)
        ti = title_str(e)
        ve = venue_str(e, kind)
        yr = year(e)
        ln = link_str(e)
        s = f"{i}. {au}, _{ti}_"
        if ve: s += f", **{ve}**"
        if yr: s += f", {yr}"
        if ln: s += f" {ln}"
        lines.append(s + "\n")
    return lines

def main():
    entries = read_bib()

    # buckets
    pre = [e for e in entries if is_preprint(e)]
    jnl = [e for e in entries if is_journal(e)]
    chp = [e for e in entries if is_chapter(e)]
    cnf = [e for e in entries if is_conf(e)]

    # page
    out = []
    out.append("---"); out.append("layout: page"); out.append("title: Publications"); out.append("---\n")
    out.extend(render_list("Preprint",       pre, "preprint"));  out.append("\n")
    out.extend(render_list("Journals",       jnl, "journal"));   out.append("\n")
    out.extend(render_list("Book Chapters",  chp, "chapter"));   out.append("\n")
    out.extend(render_list("Conferences",    cnf, "conf"))

    OUT_MD.parent.mkdir(parents=True, exist_ok=True)
    OUT_MD.write_text("\n".join(out).rstrip() + "\n", encoding="utf-8")

    # optional JSON export
    OUT_JSON.parent.mkdir(parents=True, exist_ok=True)
    def venue_for(e):
        if is_preprint(e): return venue_str(e, "preprint")
        if is_journal(e):  return venue_str(e, "journal")
        if is_chapter(e):  return venue_str(e, "chapter")
        if is_conf(e):     return venue_str(e, "conf")
        return ""

    pubs_out = []
    for e in entries:
        pubs_out.append({
            "id":      (e.get("ID") or e.get("id") or "").strip(),
            "type":    etype(e),
            "title":   title_str(e),
            "authors": [p.strip() for p in str(e.get("author") or "").split(" and ") if p.strip()],
            "year":    year(e),
            "venue":   venue_for(e),
            "url":     (e.get("url") or ""),
            "tags":    tags_from_keywords(e),   # <--- THIS is what Explore uses
        })
    with open(OUT_JSON, "w", encoding="utf-8") as f:
        json.dump(pubs_out, f, ensure_ascii=False, indent=2)

    # topic graph for Explore (now enriched)
    OUT_GRAPH.parent.mkdir(parents=True, exist_ok=True)
    base_graph = build_topic_graph(pubs_out)
    graph = enrich_topic_graph(base_graph, pubs_out)
    with open(OUT_GRAPH, "w", encoding="utf-8") as f:
        json.dump(graph, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    main()
